{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQy_q1lfFLSc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215, 12042)\n",
      "(215, 1305)\n",
      "(215, 534)\n"
     ]
    }
   ],
   "source": [
    "## This Cell is related to date preprocessing and is completely copied from original kipf's github code!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch,torch.nn,torch.sparse,torch.nn.functional,torch.distributions\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "from input_data import load_data\n",
    "from preprocessing import preprocess_graph, sparse_to_tuple, mask_test_edges\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W_gene      =  pd.read_table( \"GBM_Adjacency_matrices/gene.txt\", delim_whitespace=True).to_numpy()\n",
    "W_methy     =  pd.read_table( \"GBM_Adjacency_matrices/methy.txt\", delim_whitespace=True).to_numpy()\n",
    "W_mirna     =  pd.read_table( \"GBM_Adjacency_matrices/mirna.txt\", delim_whitespace=True).to_numpy()\n",
    "\n",
    "gene      =  pd.read_table( \"GBM/GLIO_Gene_Expression.txt\", delim_whitespace=True).to_numpy().transpose()\n",
    "methy     =  pd.read_table( \"GBM/GLIO_Methy_Expression.txt\", delim_whitespace=True).to_numpy().transpose()\n",
    "mirna     =  pd.read_table( \"GBM/GLIO_Mirna_Expression.txt\", delim_whitespace=True).to_numpy().transpose()\n",
    "truelabel =  pd.read_table( \"GBM/GLIO_Survival.txt\", delim_whitespace=True).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "order = W_argsort.reshape(-1)-1\n",
    "\n",
    "gene = gene[order]\n",
    "methy = methy[order]\n",
    "mirna = mirna[order]\n",
    "truelabel = truelabel[order]\n",
    "\n",
    "print(gene.shape)\n",
    "print(methy.shape)\n",
    "print(mirna.shape)\n",
    "\n",
    "def get_nomralized_laplacian(a):\n",
    "    b = np.diag(np.power(1.0 / a.sum(axis=1), 0.5))\n",
    "    return (b @ a @ b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Some preprocessing\n",
    "adj = W_mirna\n",
    "features = np.c_[gene,methy]\n",
    "adj_norm = get_nomralized_laplacian(adj)\n",
    "\n",
    "\n",
    "num_features = features.shape[1]\n",
    "\n",
    "#pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "#norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "pos_weight = 1\n",
    "norm = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlVFbazxFPjJ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Model definition Cell\n",
    "\n",
    "\n",
    "adj_label_tensor = torch.Tensor(adj)\n",
    "adj_norm_tensor  = torch.Tensor(adj_norm)\n",
    "features_tensor  = torch.Tensor(features)\n",
    "\n",
    "# from the previous cell following arrays are obtained : \n",
    "#adj_label\n",
    "#adj_norm\n",
    "#features\n",
    "#val_edges_false\n",
    "#test_edges\n",
    "#test_edges_false\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,first_layer_dim=1000,embedding_dim=100,A_tilda=None ,**kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "        self.W0 = torch.nn.Linear(num_features, first_layer_dim)\n",
    "        self.W1_mean = torch.nn.Linear(first_layer_dim, embedding_dim)\n",
    "        self.W1_log_std  = torch.nn.Linear(first_layer_dim, embedding_dim)\n",
    "\n",
    "        self.A_tilda = A_tilda\n",
    "\n",
    "        self.normal_dist = torch.distributions.MultivariateNormal(loc=torch.zeros(embedding_dim),scale_tril=torch.eye(embedding_dim))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params=list(self.W0.parameters())+list(self.W1_mean.parameters())+list(self.W1_log_std.parameters()),lr=0.01)\n",
    "        self.recon = None\n",
    "        \n",
    "\n",
    "    def train(self,x,A):\n",
    "\n",
    "\n",
    "        first_layer = torch.nn.functional.relu(torch.matmul(self.A_tilda,self.W0(x)))\n",
    "        z_mean      = torch.matmul(self.A_tilda,self.W1_mean(first_layer))\n",
    "        z_log_std   = torch.matmul(self.A_tilda,self.W1_log_std(first_layer))\n",
    "\n",
    "\n",
    "        z = z_mean +self.normal_dist.sample((x.shape[0],)) * torch.exp(z_log_std)\n",
    "        \n",
    "       \n",
    "\n",
    "        recon = torch.matmul(z,z.transpose(dim0=0,dim1=1))\n",
    "        self.recon = torch.sigmoid(recon)\n",
    "\n",
    "        recon_loss = (  A    *    torch.nn.functional.logsigmoid(recon)  ).mean()*pos_weight + ( (1-A) * torch.nn.functional.logsigmoid(-recon)).mean()\n",
    "        recon_loss = -norm*recon_loss\n",
    "\n",
    "        kl_loss =   torch.mean(torch.sum(1 + 2 * z_log_std - z_mean**2 -torch.exp(z_log_std)**2, 1))\n",
    "        kl_loss = -(0.5 / x.shape[0])*kl_loss\n",
    "\n",
    "        loss = recon_loss+kl_loss\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5thCdXN6FUej",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.86629\n",
      "Epoch: 0002 train_loss= 15.71302\n",
      "Epoch: 0003 train_loss= 5.66508\n",
      "Epoch: 0004 train_loss= 2.94591\n",
      "Epoch: 0005 train_loss= 1.88435\n",
      "Epoch: 0006 train_loss= 1.35395\n",
      "Epoch: 0007 train_loss= 1.19873\n",
      "Epoch: 0008 train_loss= 1.11769\n",
      "Epoch: 0009 train_loss= 1.18130\n",
      "Epoch: 0010 train_loss= 1.04280\n",
      "Epoch: 0011 train_loss= 0.96059\n",
      "Epoch: 0012 train_loss= 0.94309\n",
      "Epoch: 0013 train_loss= 0.95453\n",
      "Epoch: 0014 train_loss= 1.03564\n",
      "Epoch: 0015 train_loss= 1.01453\n",
      "Epoch: 0016 train_loss= 0.97239\n",
      "Epoch: 0017 train_loss= 0.95377\n",
      "Epoch: 0018 train_loss= 0.94351\n",
      "Epoch: 0019 train_loss= 0.93497\n",
      "Epoch: 0020 train_loss= 0.93949\n",
      "Epoch: 0021 train_loss= 0.92608\n",
      "Epoch: 0022 train_loss= 0.92471\n",
      "Epoch: 0023 train_loss= 0.89374\n",
      "Epoch: 0024 train_loss= 0.87462\n",
      "Epoch: 0025 train_loss= 0.85780\n",
      "Epoch: 0026 train_loss= 0.86224\n",
      "Epoch: 0027 train_loss= 0.84894\n",
      "Epoch: 0028 train_loss= 0.84985\n",
      "Epoch: 0029 train_loss= 0.83811\n",
      "Epoch: 0030 train_loss= 0.83253\n",
      "Epoch: 0031 train_loss= 0.81845\n",
      "Epoch: 0032 train_loss= 0.81504\n",
      "Epoch: 0033 train_loss= 0.81295\n",
      "Epoch: 0034 train_loss= 0.80582\n",
      "Epoch: 0035 train_loss= 0.80638\n",
      "Epoch: 0036 train_loss= 0.81002\n",
      "Epoch: 0037 train_loss= 0.79832\n",
      "Epoch: 0038 train_loss= 0.79994\n",
      "Epoch: 0039 train_loss= 0.79966\n",
      "Epoch: 0040 train_loss= 0.78923\n",
      "Epoch: 0041 train_loss= 0.78918\n",
      "Epoch: 0042 train_loss= 0.79641\n",
      "Epoch: 0043 train_loss= 0.79037\n",
      "Epoch: 0044 train_loss= 0.78988\n",
      "Epoch: 0045 train_loss= 0.78848\n",
      "Epoch: 0046 train_loss= 0.78774\n",
      "Epoch: 0047 train_loss= 0.78710\n",
      "Epoch: 0048 train_loss= 0.78775\n",
      "Epoch: 0049 train_loss= 0.78519\n",
      "Epoch: 0050 train_loss= 0.78999\n",
      "Epoch: 0051 train_loss= 0.77935\n",
      "Epoch: 0052 train_loss= 0.78162\n",
      "Epoch: 0053 train_loss= 0.77803\n",
      "Epoch: 0054 train_loss= 0.77986\n",
      "Epoch: 0055 train_loss= 0.78534\n",
      "Epoch: 0056 train_loss= 0.77284\n",
      "Epoch: 0057 train_loss= 0.77431\n",
      "Epoch: 0058 train_loss= 0.77269\n",
      "Epoch: 0059 train_loss= 0.78227\n",
      "Epoch: 0060 train_loss= 0.77287\n",
      "Epoch: 0061 train_loss= 0.77512\n",
      "Epoch: 0062 train_loss= 0.77838\n",
      "Epoch: 0063 train_loss= 0.77360\n",
      "Epoch: 0064 train_loss= 0.77269\n",
      "Epoch: 0065 train_loss= 0.77568\n",
      "Epoch: 0066 train_loss= 0.77471\n",
      "Epoch: 0067 train_loss= 0.77513\n",
      "Epoch: 0068 train_loss= 0.77287\n",
      "Epoch: 0069 train_loss= 0.77306\n",
      "Epoch: 0070 train_loss= 0.78364\n",
      "Epoch: 0071 train_loss= 0.77395\n",
      "Epoch: 0072 train_loss= 0.77420\n",
      "Epoch: 0073 train_loss= 0.77875\n",
      "Epoch: 0074 train_loss= 0.77622\n",
      "Epoch: 0075 train_loss= 0.77366\n",
      "Epoch: 0076 train_loss= 0.76907\n",
      "Epoch: 0077 train_loss= 0.77532\n",
      "Epoch: 0078 train_loss= 0.77649\n",
      "Epoch: 0079 train_loss= 0.77997\n",
      "Epoch: 0080 train_loss= 0.77339\n",
      "Epoch: 0081 train_loss= 0.77069\n",
      "Epoch: 0082 train_loss= 0.77203\n",
      "Epoch: 0083 train_loss= 0.77629\n",
      "Epoch: 0084 train_loss= 0.78312\n",
      "Epoch: 0085 train_loss= 0.77190\n",
      "Epoch: 0086 train_loss= 0.77536\n",
      "Epoch: 0087 train_loss= 0.77214\n",
      "Epoch: 0088 train_loss= 0.76999\n",
      "Epoch: 0089 train_loss= 0.77133\n",
      "Epoch: 0090 train_loss= 0.77274\n",
      "Epoch: 0091 train_loss= 0.77516\n",
      "Epoch: 0092 train_loss= 0.77632\n",
      "Epoch: 0093 train_loss= 0.77379\n",
      "Epoch: 0094 train_loss= 0.77178\n",
      "Epoch: 0095 train_loss= 0.77524\n",
      "Epoch: 0096 train_loss= 0.77471\n",
      "Epoch: 0097 train_loss= 0.77311\n",
      "Epoch: 0098 train_loss= 0.76655\n",
      "Epoch: 0099 train_loss= 0.77687\n",
      "Epoch: 0100 train_loss= 0.77763\n",
      "Epoch: 0101 train_loss= 0.76925\n",
      "Epoch: 0102 train_loss= 0.76940\n",
      "Epoch: 0103 train_loss= 0.77511\n",
      "Epoch: 0104 train_loss= 0.77660\n",
      "Epoch: 0105 train_loss= 0.77136\n",
      "Epoch: 0106 train_loss= 0.77154\n",
      "Epoch: 0107 train_loss= 0.77376\n",
      "Epoch: 0108 train_loss= 0.77285\n",
      "Epoch: 0109 train_loss= 0.77214\n",
      "Epoch: 0110 train_loss= 0.76991\n",
      "Epoch: 0111 train_loss= 0.77391\n",
      "Epoch: 0112 train_loss= 0.77352\n",
      "Epoch: 0113 train_loss= 0.76935\n",
      "Epoch: 0114 train_loss= 0.77201\n",
      "Epoch: 0115 train_loss= 0.77531\n",
      "Epoch: 0116 train_loss= 0.77093\n",
      "Epoch: 0117 train_loss= 0.77068\n",
      "Epoch: 0118 train_loss= 0.77066\n",
      "Epoch: 0119 train_loss= 0.77574\n",
      "Epoch: 0120 train_loss= 0.76838\n",
      "Epoch: 0121 train_loss= 0.77282\n",
      "Epoch: 0122 train_loss= 0.77150\n",
      "Epoch: 0123 train_loss= 0.77499\n",
      "Epoch: 0124 train_loss= 0.77496\n",
      "Epoch: 0125 train_loss= 0.77086\n",
      "Epoch: 0126 train_loss= 0.77267\n",
      "Epoch: 0127 train_loss= 0.76807\n",
      "Epoch: 0128 train_loss= 0.76945\n",
      "Epoch: 0129 train_loss= 0.76988\n",
      "Epoch: 0130 train_loss= 0.77511\n",
      "Epoch: 0131 train_loss= 0.77043\n",
      "Epoch: 0132 train_loss= 0.77073\n",
      "Epoch: 0133 train_loss= 0.77015\n",
      "Epoch: 0134 train_loss= 0.77057\n",
      "Epoch: 0135 train_loss= 0.76888\n",
      "Epoch: 0136 train_loss= 0.77544\n",
      "Epoch: 0137 train_loss= 0.77490\n",
      "Epoch: 0138 train_loss= 0.77090\n",
      "Epoch: 0139 train_loss= 0.77434\n",
      "Epoch: 0140 train_loss= 0.77322\n",
      "Epoch: 0141 train_loss= 0.77255\n",
      "Epoch: 0142 train_loss= 0.77483\n",
      "Epoch: 0143 train_loss= 0.76888\n",
      "Epoch: 0144 train_loss= 0.77076\n",
      "Epoch: 0145 train_loss= 0.77473\n",
      "Epoch: 0146 train_loss= 0.78136\n",
      "Epoch: 0147 train_loss= 0.76884\n",
      "Epoch: 0148 train_loss= 0.77493\n",
      "Epoch: 0149 train_loss= 0.77260\n",
      "Epoch: 0150 train_loss= 0.77256\n",
      "Epoch: 0151 train_loss= 0.77068\n",
      "Epoch: 0152 train_loss= 0.77376\n",
      "Epoch: 0153 train_loss= 0.77251\n",
      "Epoch: 0154 train_loss= 0.76942\n",
      "Epoch: 0155 train_loss= 0.77269\n",
      "Epoch: 0156 train_loss= 0.77515\n",
      "Epoch: 0157 train_loss= 0.77022\n",
      "Epoch: 0158 train_loss= 0.77095\n",
      "Epoch: 0159 train_loss= 0.77313\n",
      "Epoch: 0160 train_loss= 0.77188\n",
      "Epoch: 0161 train_loss= 0.76974\n",
      "Epoch: 0162 train_loss= 0.76672\n",
      "Epoch: 0163 train_loss= 0.77242\n",
      "Epoch: 0164 train_loss= 0.77168\n",
      "Epoch: 0165 train_loss= 0.77220\n",
      "Epoch: 0166 train_loss= 0.77349\n",
      "Epoch: 0167 train_loss= 0.76808\n",
      "Epoch: 0168 train_loss= 0.77411\n",
      "Epoch: 0169 train_loss= 0.76907\n",
      "Epoch: 0170 train_loss= 0.77079\n",
      "Epoch: 0171 train_loss= 0.77097\n",
      "Epoch: 0172 train_loss= 0.76706\n",
      "Epoch: 0173 train_loss= 0.76832\n",
      "Epoch: 0174 train_loss= 0.77122\n",
      "Epoch: 0175 train_loss= 0.76606\n",
      "Epoch: 0176 train_loss= 0.76806\n",
      "Epoch: 0177 train_loss= 0.76746\n",
      "Epoch: 0178 train_loss= 0.76885\n",
      "Epoch: 0179 train_loss= 0.76823\n",
      "Epoch: 0180 train_loss= 0.76868\n",
      "Epoch: 0181 train_loss= 0.77161\n",
      "Epoch: 0182 train_loss= 0.76773\n",
      "Epoch: 0183 train_loss= 0.76587\n",
      "Epoch: 0184 train_loss= 0.77048\n",
      "Epoch: 0185 train_loss= 0.77088\n",
      "Epoch: 0186 train_loss= 0.77317\n",
      "Epoch: 0187 train_loss= 0.76890\n",
      "Epoch: 0188 train_loss= 0.76890\n",
      "Epoch: 0189 train_loss= 0.76899\n",
      "Epoch: 0190 train_loss= 0.76824\n",
      "Epoch: 0191 train_loss= 0.76855\n",
      "Epoch: 0192 train_loss= 0.76920\n",
      "Epoch: 0193 train_loss= 0.76437\n",
      "Epoch: 0194 train_loss= 0.76963\n",
      "Epoch: 0195 train_loss= 0.77486\n",
      "Epoch: 0196 train_loss= 0.76880\n",
      "Epoch: 0197 train_loss= 0.76917\n",
      "Epoch: 0198 train_loss= 0.76869\n",
      "Epoch: 0199 train_loss= 0.76711\n",
      "Epoch: 0200 train_loss= 0.77095\n",
      "\n",
      " ######################################################### \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training Cell \n",
    "\n",
    "my_model = Model(first_layer_dim=30,embedding_dim=15,A_tilda=adj_norm_tensor)\n",
    "\n",
    "\n",
    "for epoch_counter in range(200):\n",
    "    \n",
    "    epoch_loss = my_model.train(features_tensor,adj_label_tensor)\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch_counter + 1), \"train_loss=\", \"{:.5f}\".format(epoch_loss))\n",
    "\n",
    "\n",
    "print('\\n',\"#########################################################\",'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8781, 0.6112, 0.4715,  ..., 0.6196, 0.5075, 0.3729],\n",
      "        [0.6112, 0.7133, 0.4720,  ..., 0.6187, 0.5530, 0.5472],\n",
      "        [0.4715, 0.4720, 0.7687,  ..., 0.5776, 0.4099, 0.3930],\n",
      "        ...,\n",
      "        [0.6196, 0.6187, 0.5776,  ..., 0.7642, 0.5344, 0.5138],\n",
      "        [0.5075, 0.5530, 0.4099,  ..., 0.5344, 0.9167, 0.4891],\n",
      "        [0.3729, 0.5472, 0.3930,  ..., 0.5138, 0.4891, 0.7880]],\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(my_model.recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"gvae_fusion_results/res.txt\",my_model.recon.detach().numpy(),delimiter='\\t')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kipf_VGAE_Citation_Graph.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
