{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQy_q1lfFLSc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## This Cell is related to date preprocessing and is completely copied from original kipf's github code!\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch,torch.nn,torch.sparse,torch.nn.functional,torch.distributions\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "from input_data import load_data\n",
    "from preprocessing import preprocess_graph, sparse_to_tuple, mask_test_edges\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "\n",
    "adj, features = load_data(\"cora\")\n",
    "\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "\n",
    "###################################################################### feature less training\n",
    "#features = sp.identity(features.shape[0])  # featureless\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "\n",
    "\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlVFbazxFPjJ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Model definition Cell\n",
    "\n",
    "adj_orig_tensor = torch.Tensor(adj_orig.toarray())\n",
    "\n",
    "test_edges_false = np.array(test_edges_false)\n",
    "val_edges_false = np.array(val_edges_false)\n",
    "adj_label_tensor = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].transpose()),torch.FloatTensor(adj_label[1]),torch.Size(adj_label[2])).to_dense()\n",
    "adj_norm_tensor  = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].transpose()) ,torch.FloatTensor(adj_norm[1]), torch.Size(adj_norm[2])).to_dense()\n",
    "features_tensor  = torch.sparse.FloatTensor(torch.LongTensor(features[0].transpose()) ,torch.FloatTensor(features[1]), torch.Size(features[2])).to_dense()\n",
    "\n",
    "# from the previous cell following arrays are obtained : \n",
    "#adj_label\n",
    "#adj_norm\n",
    "#features\n",
    "#val_edges_false\n",
    "#test_edges\n",
    "#test_edges_false\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,first_layer_dim=30,embedding_dim=15,A_tilda=None ,**kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "        self.W0 = torch.nn.Linear(num_features, first_layer_dim)\n",
    "        self.W1_mean = torch.nn.Linear(first_layer_dim, embedding_dim)\n",
    "        self.W1_log_std  = torch.nn.Linear(first_layer_dim, embedding_dim)\n",
    "\n",
    "        self.A_tilda = A_tilda\n",
    "\n",
    "        self.normal_dist = torch.distributions.MultivariateNormal(loc=torch.zeros(embedding_dim),scale_tril=torch.eye(embedding_dim))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params=list(self.W0.parameters())+list(self.W1_mean.parameters())+list(self.W1_log_std.parameters()),lr=0.01)\n",
    "        self.recon = None\n",
    "        \n",
    "\n",
    "    def train(self,x,A):\n",
    "\n",
    "\n",
    "        first_layer = torch.nn.functional.relu(torch.matmul(self.A_tilda,self.W0(x)))\n",
    "        z_mean      = torch.matmul(self.A_tilda,self.W1_mean(first_layer))\n",
    "        z_log_std   = torch.matmul(self.A_tilda,self.W1_log_std(first_layer))\n",
    "\n",
    "\n",
    "        z = z_mean +self.normal_dist.sample((x.shape[0],)) * torch.exp(z_log_std)\n",
    "        \n",
    "       \n",
    "\n",
    "        recon = torch.matmul(z,z.transpose(dim0=0,dim1=1))\n",
    "        self.recon = recon\n",
    "\n",
    "        recon_loss = (  A    *    torch.nn.functional.logsigmoid(recon)  ).mean()*pos_weight + ( (1-A) * torch.nn.functional.logsigmoid(-recon)).mean()\n",
    "        recon_loss = -norm*recon_loss\n",
    "\n",
    "        kl_loss =   torch.mean(torch.sum(1 + 2 * z_log_std - z_mean**2 -torch.exp(z_log_std)**2, 1))\n",
    "        kl_loss = -(0.5 / x.shape[0])*kl_loss\n",
    "\n",
    "        loss = recon_loss+kl_loss\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def evaluate(self,pos_edge,neg_edge):\n",
    "        pred_pos_edge = self.recon[pos_edge[:, 0].reshape(-1), pos_edge[:, 1].reshape(-1)].sigmoid().detach().numpy()\n",
    "        pred_neg_edge = self.recon[neg_edge[:, 0].reshape(-1), neg_edge[:, 1].reshape(-1)].sigmoid().detach().numpy()\n",
    "\n",
    "        labels = np.r_[np.ones_like(pred_pos_edge), np.zeros_like(pred_neg_edge)]\n",
    "        preds  = np.r_[pred_pos_edge, pred_neg_edge]\n",
    "\n",
    "        roc_auc = roc_auc_score(labels,preds)\n",
    "        precision=average_precision_score(labels, preds)\n",
    "        \n",
    "        \n",
    "\n",
    "        return roc_auc,precision,np.c_[labels,preds]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5thCdXN6FUej",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.77748 val_roc_auc= 0.51048 val_precision= 0.49425\n",
      "Epoch: 0002 train_loss= 1.44978 val_roc_auc= 0.51825 val_precision= 0.52862\n",
      "Epoch: 0003 train_loss= 1.18505 val_roc_auc= 0.51115 val_precision= 0.48601\n",
      "Epoch: 0004 train_loss= 0.99368 val_roc_auc= 0.54994 val_precision= 0.52709\n",
      "Epoch: 0005 train_loss= 0.86613 val_roc_auc= 0.53424 val_precision= 0.52547\n",
      "Epoch: 0006 train_loss= 0.76945 val_roc_auc= 0.56537 val_precision= 0.55766\n",
      "Epoch: 0007 train_loss= 0.73547 val_roc_auc= 0.56414 val_precision= 0.57273\n",
      "Epoch: 0008 train_loss= 0.73151 val_roc_auc= 0.59080 val_precision= 0.60212\n",
      "Epoch: 0009 train_loss= 0.74040 val_roc_auc= 0.62203 val_precision= 0.64381\n",
      "Epoch: 0010 train_loss= 0.74793 val_roc_auc= 0.63273 val_precision= 0.65965\n",
      "Epoch: 0011 train_loss= 0.74620 val_roc_auc= 0.63069 val_precision= 0.65988\n",
      "Epoch: 0012 train_loss= 0.74412 val_roc_auc= 0.64859 val_precision= 0.68360\n",
      "Epoch: 0013 train_loss= 0.74468 val_roc_auc= 0.64603 val_precision= 0.67634\n",
      "Epoch: 0014 train_loss= 0.74321 val_roc_auc= 0.71273 val_precision= 0.73861\n",
      "Epoch: 0015 train_loss= 0.74306 val_roc_auc= 0.68525 val_precision= 0.70241\n",
      "Epoch: 0016 train_loss= 0.73933 val_roc_auc= 0.71737 val_precision= 0.74732\n",
      "Epoch: 0017 train_loss= 0.73324 val_roc_auc= 0.70004 val_precision= 0.73717\n",
      "Epoch: 0018 train_loss= 0.72482 val_roc_auc= 0.73156 val_precision= 0.76355\n",
      "Epoch: 0019 train_loss= 0.71626 val_roc_auc= 0.74168 val_precision= 0.76993\n",
      "Epoch: 0020 train_loss= 0.70370 val_roc_auc= 0.78064 val_precision= 0.79432\n",
      "Epoch: 0021 train_loss= 0.68908 val_roc_auc= 0.79187 val_precision= 0.81198\n",
      "Epoch: 0022 train_loss= 0.67143 val_roc_auc= 0.80991 val_precision= 0.82282\n",
      "Epoch: 0023 train_loss= 0.65215 val_roc_auc= 0.80877 val_precision= 0.81702\n",
      "Epoch: 0024 train_loss= 0.63296 val_roc_auc= 0.82391 val_precision= 0.82737\n",
      "Epoch: 0025 train_loss= 0.61458 val_roc_auc= 0.82097 val_precision= 0.82763\n",
      "Epoch: 0026 train_loss= 0.59900 val_roc_auc= 0.81500 val_precision= 0.82359\n",
      "Epoch: 0027 train_loss= 0.58853 val_roc_auc= 0.81438 val_precision= 0.82256\n",
      "Epoch: 0028 train_loss= 0.58264 val_roc_auc= 0.82909 val_precision= 0.83539\n",
      "Epoch: 0029 train_loss= 0.58142 val_roc_auc= 0.82924 val_precision= 0.83341\n",
      "Epoch: 0030 train_loss= 0.57946 val_roc_auc= 0.83439 val_precision= 0.83721\n",
      "Epoch: 0031 train_loss= 0.57591 val_roc_auc= 0.83627 val_precision= 0.84286\n",
      "Epoch: 0032 train_loss= 0.56959 val_roc_auc= 0.84442 val_precision= 0.85360\n",
      "Epoch: 0033 train_loss= 0.56297 val_roc_auc= 0.85015 val_precision= 0.85603\n",
      "Epoch: 0034 train_loss= 0.55459 val_roc_auc= 0.84844 val_precision= 0.85703\n",
      "Epoch: 0035 train_loss= 0.54282 val_roc_auc= 0.86151 val_precision= 0.86828\n",
      "Epoch: 0036 train_loss= 0.53416 val_roc_auc= 0.85901 val_precision= 0.86293\n",
      "Epoch: 0037 train_loss= 0.52870 val_roc_auc= 0.87447 val_precision= 0.88305\n",
      "Epoch: 0038 train_loss= 0.52289 val_roc_auc= 0.88463 val_precision= 0.88911\n",
      "Epoch: 0039 train_loss= 0.51797 val_roc_auc= 0.88554 val_precision= 0.88884\n",
      "Epoch: 0040 train_loss= 0.51398 val_roc_auc= 0.89330 val_precision= 0.89827\n",
      "Epoch: 0041 train_loss= 0.51132 val_roc_auc= 0.89889 val_precision= 0.90300\n",
      "Epoch: 0042 train_loss= 0.50934 val_roc_auc= 0.89615 val_precision= 0.90248\n",
      "Epoch: 0043 train_loss= 0.50888 val_roc_auc= 0.90224 val_precision= 0.90672\n",
      "Epoch: 0044 train_loss= 0.50797 val_roc_auc= 0.90807 val_precision= 0.91386\n",
      "Epoch: 0045 train_loss= 0.50606 val_roc_auc= 0.91198 val_precision= 0.91585\n",
      "Epoch: 0046 train_loss= 0.50432 val_roc_auc= 0.90263 val_precision= 0.90603\n",
      "Epoch: 0047 train_loss= 0.50170 val_roc_auc= 0.90172 val_precision= 0.90487\n",
      "Epoch: 0048 train_loss= 0.49938 val_roc_auc= 0.90406 val_precision= 0.90788\n",
      "Epoch: 0049 train_loss= 0.49707 val_roc_auc= 0.91025 val_precision= 0.91336\n",
      "Epoch: 0050 train_loss= 0.49532 val_roc_auc= 0.89656 val_precision= 0.90018\n",
      "Epoch: 0051 train_loss= 0.49360 val_roc_auc= 0.89873 val_precision= 0.90285\n",
      "Epoch: 0052 train_loss= 0.49262 val_roc_auc= 0.90384 val_precision= 0.90750\n",
      "Epoch: 0053 train_loss= 0.49267 val_roc_auc= 0.90642 val_precision= 0.90707\n",
      "Epoch: 0054 train_loss= 0.49265 val_roc_auc= 0.90500 val_precision= 0.90653\n",
      "Epoch: 0055 train_loss= 0.49053 val_roc_auc= 0.90279 val_precision= 0.90549\n",
      "Epoch: 0056 train_loss= 0.48988 val_roc_auc= 0.91331 val_precision= 0.91342\n",
      "Epoch: 0057 train_loss= 0.48848 val_roc_auc= 0.90442 val_precision= 0.90359\n",
      "Epoch: 0058 train_loss= 0.48801 val_roc_auc= 0.91391 val_precision= 0.91602\n",
      "Epoch: 0059 train_loss= 0.48776 val_roc_auc= 0.90337 val_precision= 0.90888\n",
      "Epoch: 0060 train_loss= 0.48660 val_roc_auc= 0.91605 val_precision= 0.91817\n",
      "Epoch: 0061 train_loss= 0.48401 val_roc_auc= 0.91752 val_precision= 0.91738\n",
      "Epoch: 0062 train_loss= 0.48410 val_roc_auc= 0.91356 val_precision= 0.91588\n",
      "Epoch: 0063 train_loss= 0.48298 val_roc_auc= 0.90833 val_precision= 0.91152\n",
      "Epoch: 0064 train_loss= 0.48237 val_roc_auc= 0.91589 val_precision= 0.92242\n",
      "Epoch: 0065 train_loss= 0.48149 val_roc_auc= 0.91675 val_precision= 0.91833\n",
      "Epoch: 0066 train_loss= 0.48074 val_roc_auc= 0.90963 val_precision= 0.91451\n",
      "Epoch: 0067 train_loss= 0.48007 val_roc_auc= 0.92043 val_precision= 0.92340\n",
      "Epoch: 0068 train_loss= 0.47899 val_roc_auc= 0.91691 val_precision= 0.92300\n",
      "Epoch: 0069 train_loss= 0.47788 val_roc_auc= 0.91067 val_precision= 0.91891\n",
      "Epoch: 0070 train_loss= 0.47669 val_roc_auc= 0.91661 val_precision= 0.92149\n",
      "Epoch: 0071 train_loss= 0.47617 val_roc_auc= 0.91535 val_precision= 0.92143\n",
      "Epoch: 0072 train_loss= 0.47533 val_roc_auc= 0.91787 val_precision= 0.91948\n",
      "Epoch: 0073 train_loss= 0.47490 val_roc_auc= 0.91349 val_precision= 0.91907\n",
      "Epoch: 0074 train_loss= 0.47399 val_roc_auc= 0.90966 val_precision= 0.91733\n",
      "Epoch: 0075 train_loss= 0.47348 val_roc_auc= 0.91900 val_precision= 0.91949\n",
      "Epoch: 0076 train_loss= 0.47219 val_roc_auc= 0.91619 val_precision= 0.92120\n",
      "Epoch: 0077 train_loss= 0.47176 val_roc_auc= 0.91350 val_precision= 0.91293\n",
      "Epoch: 0078 train_loss= 0.47097 val_roc_auc= 0.91404 val_precision= 0.91923\n",
      "Epoch: 0079 train_loss= 0.47006 val_roc_auc= 0.90682 val_precision= 0.91364\n",
      "Epoch: 0080 train_loss= 0.46970 val_roc_auc= 0.91774 val_precision= 0.91861\n",
      "Epoch: 0081 train_loss= 0.46857 val_roc_auc= 0.91343 val_precision= 0.91793\n",
      "Epoch: 0082 train_loss= 0.46751 val_roc_auc= 0.90580 val_precision= 0.91132\n",
      "Epoch: 0083 train_loss= 0.46675 val_roc_auc= 0.91151 val_precision= 0.91428\n",
      "Epoch: 0084 train_loss= 0.46619 val_roc_auc= 0.91200 val_precision= 0.92009\n",
      "Epoch: 0085 train_loss= 0.46673 val_roc_auc= 0.91563 val_precision= 0.92189\n",
      "Epoch: 0086 train_loss= 0.46565 val_roc_auc= 0.91930 val_precision= 0.92593\n",
      "Epoch: 0087 train_loss= 0.46426 val_roc_auc= 0.90862 val_precision= 0.91685\n",
      "Epoch: 0088 train_loss= 0.46402 val_roc_auc= 0.90702 val_precision= 0.91239\n",
      "Epoch: 0089 train_loss= 0.46318 val_roc_auc= 0.90809 val_precision= 0.92092\n",
      "Epoch: 0090 train_loss= 0.46336 val_roc_auc= 0.91800 val_precision= 0.92397\n",
      "Epoch: 0091 train_loss= 0.46284 val_roc_auc= 0.90369 val_precision= 0.91379\n",
      "Epoch: 0092 train_loss= 0.46162 val_roc_auc= 0.91405 val_precision= 0.92065\n",
      "Epoch: 0093 train_loss= 0.46163 val_roc_auc= 0.91227 val_precision= 0.92091\n",
      "Epoch: 0094 train_loss= 0.45996 val_roc_auc= 0.90966 val_precision= 0.92043\n",
      "Epoch: 0095 train_loss= 0.46023 val_roc_auc= 0.90114 val_precision= 0.91685\n",
      "Epoch: 0096 train_loss= 0.45957 val_roc_auc= 0.91603 val_precision= 0.92431\n",
      "Epoch: 0097 train_loss= 0.45940 val_roc_auc= 0.91104 val_precision= 0.92328\n",
      "Epoch: 0098 train_loss= 0.45904 val_roc_auc= 0.91725 val_precision= 0.92680\n",
      "Epoch: 0099 train_loss= 0.45820 val_roc_auc= 0.91224 val_precision= 0.92002\n",
      "Epoch: 0100 train_loss= 0.45863 val_roc_auc= 0.91051 val_precision= 0.91823\n",
      "Epoch: 0101 train_loss= 0.45754 val_roc_auc= 0.90834 val_precision= 0.92053\n",
      "Epoch: 0102 train_loss= 0.45760 val_roc_auc= 0.89993 val_precision= 0.91371\n",
      "Epoch: 0103 train_loss= 0.45713 val_roc_auc= 0.90240 val_precision= 0.91883\n",
      "Epoch: 0104 train_loss= 0.45656 val_roc_auc= 0.90840 val_precision= 0.92049\n",
      "Epoch: 0105 train_loss= 0.45665 val_roc_auc= 0.90901 val_precision= 0.92294\n",
      "Epoch: 0106 train_loss= 0.45563 val_roc_auc= 0.91045 val_precision= 0.92485\n",
      "Epoch: 0107 train_loss= 0.45578 val_roc_auc= 0.90992 val_precision= 0.91946\n",
      "Epoch: 0108 train_loss= 0.45526 val_roc_auc= 0.90835 val_precision= 0.92240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 train_loss= 0.45485 val_roc_auc= 0.90457 val_precision= 0.92036\n",
      "Epoch: 0110 train_loss= 0.45489 val_roc_auc= 0.91826 val_precision= 0.93094\n",
      "Epoch: 0111 train_loss= 0.45449 val_roc_auc= 0.90886 val_precision= 0.91935\n",
      "Epoch: 0112 train_loss= 0.45416 val_roc_auc= 0.90786 val_precision= 0.92223\n",
      "Epoch: 0113 train_loss= 0.45397 val_roc_auc= 0.91149 val_precision= 0.92413\n",
      "Epoch: 0114 train_loss= 0.45375 val_roc_auc= 0.91428 val_precision= 0.92718\n",
      "Epoch: 0115 train_loss= 0.45321 val_roc_auc= 0.90460 val_precision= 0.92323\n",
      "Epoch: 0116 train_loss= 0.45285 val_roc_auc= 0.91470 val_precision= 0.92907\n",
      "Epoch: 0117 train_loss= 0.45303 val_roc_auc= 0.91509 val_precision= 0.92713\n",
      "Epoch: 0118 train_loss= 0.45236 val_roc_auc= 0.91482 val_precision= 0.92800\n",
      "Epoch: 0119 train_loss= 0.45213 val_roc_auc= 0.91488 val_precision= 0.93028\n",
      "Epoch: 0120 train_loss= 0.45197 val_roc_auc= 0.91506 val_precision= 0.92959\n",
      "Epoch: 0121 train_loss= 0.45162 val_roc_auc= 0.91673 val_precision= 0.92905\n",
      "Epoch: 0122 train_loss= 0.45141 val_roc_auc= 0.91448 val_precision= 0.92667\n",
      "Epoch: 0123 train_loss= 0.45115 val_roc_auc= 0.91178 val_precision= 0.92632\n",
      "Epoch: 0124 train_loss= 0.45086 val_roc_auc= 0.90976 val_precision= 0.92466\n",
      "Epoch: 0125 train_loss= 0.45086 val_roc_auc= 0.91038 val_precision= 0.92493\n",
      "Epoch: 0126 train_loss= 0.44984 val_roc_auc= 0.91728 val_precision= 0.92789\n",
      "Epoch: 0127 train_loss= 0.44981 val_roc_auc= 0.90428 val_precision= 0.92174\n",
      "Epoch: 0128 train_loss= 0.44947 val_roc_auc= 0.91904 val_precision= 0.93212\n",
      "Epoch: 0129 train_loss= 0.44956 val_roc_auc= 0.91203 val_precision= 0.92699\n",
      "Epoch: 0130 train_loss= 0.44855 val_roc_auc= 0.91047 val_precision= 0.92442\n",
      "Epoch: 0131 train_loss= 0.44865 val_roc_auc= 0.92102 val_precision= 0.93127\n",
      "Epoch: 0132 train_loss= 0.44808 val_roc_auc= 0.91511 val_precision= 0.92629\n",
      "Epoch: 0133 train_loss= 0.44787 val_roc_auc= 0.92260 val_precision= 0.93394\n",
      "Epoch: 0134 train_loss= 0.44716 val_roc_auc= 0.91600 val_precision= 0.93169\n",
      "Epoch: 0135 train_loss= 0.44696 val_roc_auc= 0.91941 val_precision= 0.93091\n",
      "Epoch: 0136 train_loss= 0.44679 val_roc_auc= 0.91412 val_precision= 0.93065\n",
      "Epoch: 0137 train_loss= 0.44582 val_roc_auc= 0.91156 val_precision= 0.92826\n",
      "Epoch: 0138 train_loss= 0.44592 val_roc_auc= 0.91923 val_precision= 0.93565\n",
      "Epoch: 0139 train_loss= 0.44556 val_roc_auc= 0.91787 val_precision= 0.93236\n",
      "Epoch: 0140 train_loss= 0.44539 val_roc_auc= 0.91470 val_precision= 0.92904\n",
      "Epoch: 0141 train_loss= 0.44508 val_roc_auc= 0.90525 val_precision= 0.92348\n",
      "Epoch: 0142 train_loss= 0.44474 val_roc_auc= 0.91272 val_precision= 0.93092\n",
      "Epoch: 0143 train_loss= 0.44460 val_roc_auc= 0.92261 val_precision= 0.93668\n",
      "Epoch: 0144 train_loss= 0.44411 val_roc_auc= 0.91222 val_precision= 0.92866\n",
      "Epoch: 0145 train_loss= 0.44517 val_roc_auc= 0.91969 val_precision= 0.93237\n",
      "Epoch: 0146 train_loss= 0.44475 val_roc_auc= 0.91887 val_precision= 0.93350\n",
      "Epoch: 0147 train_loss= 0.44335 val_roc_auc= 0.92434 val_precision= 0.93895\n",
      "Epoch: 0148 train_loss= 0.44269 val_roc_auc= 0.91022 val_precision= 0.92764\n",
      "Epoch: 0149 train_loss= 0.44359 val_roc_auc= 0.91516 val_precision= 0.93178\n",
      "Epoch: 0150 train_loss= 0.44270 val_roc_auc= 0.91813 val_precision= 0.93250\n",
      "Epoch: 0151 train_loss= 0.44234 val_roc_auc= 0.92381 val_precision= 0.93860\n",
      "Epoch: 0152 train_loss= 0.44240 val_roc_auc= 0.91808 val_precision= 0.93268\n",
      "Epoch: 0153 train_loss= 0.44165 val_roc_auc= 0.91567 val_precision= 0.93403\n",
      "Epoch: 0154 train_loss= 0.44227 val_roc_auc= 0.91687 val_precision= 0.93420\n",
      "Epoch: 0155 train_loss= 0.44203 val_roc_auc= 0.91207 val_precision= 0.92967\n",
      "Epoch: 0156 train_loss= 0.44104 val_roc_auc= 0.91660 val_precision= 0.93152\n",
      "Epoch: 0157 train_loss= 0.44147 val_roc_auc= 0.92450 val_precision= 0.93833\n",
      "Epoch: 0158 train_loss= 0.44077 val_roc_auc= 0.91885 val_precision= 0.93339\n",
      "Epoch: 0159 train_loss= 0.44050 val_roc_auc= 0.92491 val_precision= 0.93879\n",
      "Epoch: 0160 train_loss= 0.44067 val_roc_auc= 0.92085 val_precision= 0.93523\n",
      "Epoch: 0161 train_loss= 0.44010 val_roc_auc= 0.91677 val_precision= 0.93338\n",
      "Epoch: 0162 train_loss= 0.44048 val_roc_auc= 0.91190 val_precision= 0.93326\n",
      "Epoch: 0163 train_loss= 0.44016 val_roc_auc= 0.91165 val_precision= 0.92898\n",
      "Epoch: 0164 train_loss= 0.43993 val_roc_auc= 0.92194 val_precision= 0.93938\n",
      "Epoch: 0165 train_loss= 0.43977 val_roc_auc= 0.92184 val_precision= 0.93768\n",
      "Epoch: 0166 train_loss= 0.43979 val_roc_auc= 0.91800 val_precision= 0.93574\n",
      "Epoch: 0167 train_loss= 0.43928 val_roc_auc= 0.92643 val_precision= 0.93826\n",
      "Epoch: 0168 train_loss= 0.43932 val_roc_auc= 0.91865 val_precision= 0.93365\n",
      "Epoch: 0169 train_loss= 0.43877 val_roc_auc= 0.92553 val_precision= 0.94027\n",
      "Epoch: 0170 train_loss= 0.43897 val_roc_auc= 0.92180 val_precision= 0.93674\n",
      "Epoch: 0171 train_loss= 0.43896 val_roc_auc= 0.91648 val_precision= 0.93523\n",
      "Epoch: 0172 train_loss= 0.43892 val_roc_auc= 0.92316 val_precision= 0.93795\n",
      "Epoch: 0173 train_loss= 0.43854 val_roc_auc= 0.92536 val_precision= 0.94150\n",
      "Epoch: 0174 train_loss= 0.43829 val_roc_auc= 0.92446 val_precision= 0.93920\n",
      "Epoch: 0175 train_loss= 0.43833 val_roc_auc= 0.91752 val_precision= 0.93573\n",
      "Epoch: 0176 train_loss= 0.43808 val_roc_auc= 0.93021 val_precision= 0.94282\n",
      "Epoch: 0177 train_loss= 0.43826 val_roc_auc= 0.91525 val_precision= 0.93280\n",
      "Epoch: 0178 train_loss= 0.43807 val_roc_auc= 0.92150 val_precision= 0.93532\n",
      "Epoch: 0179 train_loss= 0.43805 val_roc_auc= 0.91122 val_precision= 0.93143\n",
      "Epoch: 0180 train_loss= 0.43795 val_roc_auc= 0.92284 val_precision= 0.93831\n",
      "Epoch: 0181 train_loss= 0.43785 val_roc_auc= 0.91933 val_precision= 0.93521\n",
      "Epoch: 0182 train_loss= 0.43807 val_roc_auc= 0.93104 val_precision= 0.94291\n",
      "Epoch: 0183 train_loss= 0.43765 val_roc_auc= 0.92643 val_precision= 0.94015\n",
      "Epoch: 0184 train_loss= 0.43753 val_roc_auc= 0.92129 val_precision= 0.93802\n",
      "Epoch: 0185 train_loss= 0.43732 val_roc_auc= 0.91021 val_precision= 0.93039\n",
      "Epoch: 0186 train_loss= 0.43765 val_roc_auc= 0.91557 val_precision= 0.93232\n",
      "Epoch: 0187 train_loss= 0.43744 val_roc_auc= 0.91853 val_precision= 0.93365\n",
      "Epoch: 0188 train_loss= 0.43716 val_roc_auc= 0.91700 val_precision= 0.93495\n",
      "Epoch: 0189 train_loss= 0.43733 val_roc_auc= 0.91501 val_precision= 0.93001\n",
      "Epoch: 0190 train_loss= 0.43714 val_roc_auc= 0.91555 val_precision= 0.93282\n",
      "Epoch: 0191 train_loss= 0.43685 val_roc_auc= 0.92073 val_precision= 0.93705\n",
      "Epoch: 0192 train_loss= 0.43682 val_roc_auc= 0.91943 val_precision= 0.93331\n",
      "Epoch: 0193 train_loss= 0.43665 val_roc_auc= 0.91512 val_precision= 0.93491\n",
      "Epoch: 0194 train_loss= 0.43634 val_roc_auc= 0.91869 val_precision= 0.93633\n",
      "Epoch: 0195 train_loss= 0.43655 val_roc_auc= 0.92105 val_precision= 0.93802\n",
      "Epoch: 0196 train_loss= 0.43628 val_roc_auc= 0.91859 val_precision= 0.93604\n",
      "Epoch: 0197 train_loss= 0.43663 val_roc_auc= 0.92500 val_precision= 0.93939\n",
      "Epoch: 0198 train_loss= 0.43622 val_roc_auc= 0.91843 val_precision= 0.93547\n",
      "Epoch: 0199 train_loss= 0.43655 val_roc_auc= 0.90834 val_precision= 0.92879\n",
      "Epoch: 0200 train_loss= 0.43597 val_roc_auc= 0.92017 val_precision= 0.93777\n",
      "\n",
      " ######################################################### \n",
      "\n",
      "test_roc_auc= 0.91318 test_precision= 0.93777\n"
     ]
    }
   ],
   "source": [
    "#Training Cell \n",
    "\n",
    "my_model = Model(first_layer_dim=30,embedding_dim=15,A_tilda=adj_norm_tensor)\n",
    "\n",
    "\n",
    "for epoch_counter in range(200):\n",
    "    \n",
    "    epoch_loss = my_model.train(features_tensor,adj_label_tensor)\n",
    "    epoch_roc_auc,epoch_precision,_ = my_model.evaluate(val_edges,val_edges_false)\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch_counter + 1), \"train_loss=\", \"{:.5f}\".format(epoch_loss), \"val_roc_auc=\", \"{:.5f}\".format(epoch_roc_auc),\n",
    "          \"val_precision=\", \"{:.5f}\".format(epoch_precision))\n",
    "\n",
    "\n",
    "print('\\n',\"#########################################################\",'\\n')\n",
    "\n",
    "test_roc_auc,test_precision,label_pred = my_model.evaluate(test_edges,test_edges_false)\n",
    "print(\"test_roc_auc=\",\"{:.5f}\".format(test_roc_auc),\"test_precision=\",\"{:.5f}\".format(epoch_precision))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7241758241758242\n"
     ]
    }
   ],
   "source": [
    "tmp = np.abs(label_pred[:,0]-np.round(label_pred[:,1]))\n",
    "\n",
    "print(1-tmp.sum()/tmp.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "#print(adj_label_tensor[val_edges[:,0],val_edges[:,1]])\n",
    "#print(adj_orig_tensor [val_edges[:,0],val_edges[:,1]])\n",
    "print(features_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2500)\n"
     ]
    }
   ],
   "source": [
    "print(adj_norm_tensor[4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kipf_VGAE_Citation_Graph.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
