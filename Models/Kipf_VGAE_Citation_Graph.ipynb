{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kipf_VGAE_Citation_Graph.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZQy_q1lfFLSc","colab_type":"code","colab":{}},"source":["## This Cell is related to date preprocessing and is completely copied from original kipf's github code!\n","\n","import numpy as np\n","import scipy.sparse as sp\n","import torch,torch.nn,torch.sparse,torch.nn.functional,torch.distributions\n","from sklearn.metrics import roc_auc_score,average_precision_score\n","from input_data import load_data\n","from preprocessing import preprocess_graph, sparse_to_tuple, mask_test_edges\n","np.set_printoptions(threshold=np.inf)\n","\n","\n","\n","adj, features = load_data(\"citeseer\")\n","\n","\n","# Store original adjacency matrix (without diagonal entries) for later\n","adj_orig = adj\n","adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n","adj_orig.eliminate_zeros()\n","\n","adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n","adj = adj_train\n","\n","\n","# Some preprocessing\n","adj_norm = preprocess_graph(adj)\n","\n","\n","num_nodes = adj.shape[0]\n","\n","features = sparse_to_tuple(features.tocoo())\n","num_features = features[2][1]\n","features_nonzero = features[1].shape[0]\n","\n","\n","\n","pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n","norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n","\n","\n","\n","adj_label = adj_train + sp.eye(adj_train.shape[0])\n","adj_label = sparse_to_tuple(adj_label)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlVFbazxFPjJ","colab_type":"code","colab":{}},"source":["#Model definition Cell\n","\n","test_edges_false = np.array(test_edges_false)\n","val_edges_false = np.array(val_edges_false)\n","adj_label_tensor = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].transpose()),torch.FloatTensor(adj_label[1]),torch.Size(adj_label[2])).to_dense()\n","adj_norm_tensor  = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].transpose()) ,torch.FloatTensor(adj_norm[1]), torch.Size(adj_norm[2])).to_dense()\n","features_tensor  = torch.sparse.FloatTensor(torch.LongTensor(features[0].transpose()) ,torch.FloatTensor(features[1]), torch.Size(features[2])).to_dense()\n","\n","# from the previous cell following arrays are obtained : \n","#adj_label\n","#adj_norm\n","#features\n","#val_edges_false\n","#test_edges\n","#test_edges_false\n","\n","\n","\n","\n","class Model(torch.nn.Module):\n","\n","    def __init__(self,first_layer_dim=30,embedding_dim=15,A_tilda=None ,**kwargs):\n","        super(Model, self).__init__(**kwargs)\n","\n","\n","        self.W0 = torch.nn.Linear(num_features, first_layer_dim)\n","        self.W1_mean = torch.nn.Linear(first_layer_dim, embedding_dim)\n","        self.W1_log_std  = torch.nn.Linear(first_layer_dim, embedding_dim)\n","\n","        self.A_tilda = A_tilda\n","\n","        self.normal_dist = torch.distributions.MultivariateNormal(loc=torch.zeros(embedding_dim),scale_tril=torch.eye(embedding_dim))\n","\n","        self.optimizer = torch.optim.Adam(params=list(self.W0.parameters())+list(self.W1_mean.parameters())+list(self.W1_log_std.parameters()),lr=0.01)\n","        self.recon = None\n","\n","\n","    def train(self,x,A):\n","\n","\n","        first_layer = torch.nn.functional.relu(torch.matmul(self.A_tilda,self.W0(x)))\n","        z_mean      = torch.matmul(self.A_tilda,self.W1_mean(first_layer))\n","        z_log_std   = torch.matmul(self.A_tilda,self.W1_log_std(first_layer))\n","\n","\n","        z = z_mean +self.normal_dist.sample((x.shape[0],)) * torch.exp(z_log_std)\n","\n","\n","        recon = torch.matmul(z,z.transpose(dim0=0,dim1=1))\n","        self.recon = recon\n","\n","        recon_loss = (  A    *    torch.nn.functional.logsigmoid(recon)  ).mean()*pos_weight + ( (1-A) * torch.nn.functional.logsigmoid(-recon)).mean()\n","        recon_loss = -norm*recon_loss\n","\n","        kl_loss = (0.5 / x.shape[0]) * torch.mean(torch.sum(1 + 2 * z_log_std - z_mean**2 -torch.exp(z_log_std)**2, 1))\n","        kl_loss = -kl_loss\n","\n","        loss = recon_loss+kl_loss\n","\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","\n","    def evaluate(self,pos_edge,neg_edge):\n","        pred_pos_edge = self.recon[pos_edge[:, 0].reshape(-1), pos_edge[:, 1].reshape(-1)].sigmoid().detach().numpy()\n","        pred_neg_edge = self.recon[neg_edge[:, 0].reshape(-1), neg_edge[:, 1].reshape(-1)].sigmoid().detach().numpy()\n","\n","        labels = np.r_[np.ones_like(pred_pos_edge), np.zeros_like(pred_neg_edge)]\n","        preds  = np.r_[pred_pos_edge, pred_neg_edge]\n","\n","        roc_auc = roc_auc_score(labels,preds)\n","        precision=average_precision_score(labels, preds)\n","\n","        return roc_auc,precision\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5thCdXN6FUej","colab_type":"code","colab":{}},"source":["#Training Cell \n","\n","my_model = Model(first_layer_dim=30,embedding_dim=15,A_tilda=adj_norm_tensor)\n","\n","\n","for epoch_counter in range(20):\n","    \n","    epoch_loss = my_model.train(features_tensor,adj_label_tensor)\n","    epoch_roc_auc,epoch_precision = my_model.evaluate(val_edges,val_edges_false)\n","    \n","    print(\"Epoch:\", '%04d' % (epoch_counter + 1), \"train_loss=\", \"{:.5f}\".format(epoch_loss), \"val_roc_auc=\", \"{:.5f}\".format(epoch_roc_auc),\n","          \"val_precision=\", \"{:.5f}\".format(epoch_precision))\n","\n","\n","print('\\n',\"#########################################################\",'\\n')\n","\n","test_roc_auc,test_precision = my_model.evaluate(test_edges,test_edges_false)\n","print(\"test_roc_auc=\",\"{:.5f}\".format(test_roc_auc),\"test_precision=\",\"{:.5f}\".format(epoch_precision))\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}